from typing import List, Iterator
import torch
import torch.nn as nn

class RotationLayer(nn.Module):
    def __init__(self, theta: float):
        super().__init__()
        self.theta = torch.tensor(theta, dtype=torch.float32)  # Store as tensor
        self.register_buffer("rotation_matrix", self._get_rotation_matrix(self.theta))

    def _get_rotation_matrix(self, theta: torch.Tensor):
        """Create a 2x2 rotation matrix for the given angle (in degrees)."""
        return torch.tensor([
            [torch.cos(theta), -torch.sin(theta)],
            [torch.sin(theta), torch.cos(theta)]], dtype=torch.float32)

    def theta_str(self):
        return f'2π*{(self.theta/(2*torch.pi)):.3f}'

    def forward(self, x):
        """
        x: Tensor of shape (batch_size, 2)
        Returns: Rotated tensor of shape (batch_size, 2)
        """
        return torch.matmul(x, self.rotation_matrix.T)  # Apply rotation

class LearnableRotationLayer(nn.Module):
    def __init__(self, theta: float):
        super().__init__()
        self.theta = nn.Parameter(torch.tensor(theta, dtype=torch.float32))  # Learnable angle in radians

    def theta_str(self):
        return f'2π*{(self.theta/(2*torch.pi)):.3f}'

    def forward(self, x):
        theta = self.theta  # Keep theta in radians
        rotation_matrix = torch.stack([
            torch.cos(theta), -torch.sin(theta),
            torch.sin(theta), torch.cos(theta)
        ]).reshape(2, 2)
        return torch.matmul(x, rotation_matrix.T)

class SimpleClassifier(nn.Module):
    """
    A simple module containing linear layers seperated by ReLu nonlinear layers, with final Softmax:
    Linear -> ReLu -> Linear -> ReLu -> ... -> Linear -> SoftMax
    """

    def __init__(self, dimensions: List[int]):
        """
        The k-th linear relu_layers (starting from 0) goes from dimensions[k] to dimensions[k+1].
        In particular, the input of the classifier is dimensions[0] (which is automatically flatten),
        and output is dimensions[-1].
        """
        assert all(d > 0 for d in dimensions)
        super(SimpleClassifier, self).__init__()
        self.progress_output = None
        self.flatten = nn.Flatten()
        self.outputs = dict()
        self._linear_layers = []

        relu_layers = []
        linear_index = 0
        for input_size, output_size in zip(dimensions[:-1], dimensions[1:]):
            linear_layer = nn.Linear(input_size, output_size)
            linear_layer.name = f'Linear_{linear_index}'
            linear_index += 1
            self._linear_layers.append(linear_layer)
            # linear_layer.register_forward_hook(self.output_hook)
            relu_layers += [
                linear_layer,
                nn.ReLU()
            ]

        # relu_layers[1] = nn.ReLU()

        self.linear_relu = nn.Sequential(
            *relu_layers[:-1]    # don't add ReLu before the last softmax
        )

        self.softmax = nn.Softmax(dim=1)

    def get_linear_layer(self, k: int = 0) -> nn.Linear:
        return self._linear_layers[k]

    def number_of_linear_layers(self) -> int:
        return len(self._linear_layers)

    def linear_layers(self) -> Iterator[nn.Linear]:
        return iter(self._linear_layers)

    def output_hook(self, module, input, output):
        self.outputs[module.name] = output

    def forward(self, x):
        x = self.flatten(x)
        x = self.linear_relu(x)
        x = self.softmax(x)
        return x

    def get_tracked_info(self):
        return [self.progress_output]